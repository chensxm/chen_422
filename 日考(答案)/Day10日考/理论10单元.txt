(1) [ 单选 ] ID:16618
对于神经网络中超参数调试方法正确的是 
A) 随机选择点来试验超参数效果
B) 当你给超参数取值时，另一个惯例是采用由精细到粗糙的策略。
C) 只调试容易的
D) 给定默认值即可
回答： 
答案： A


(2) [ 单选 ] ID:16603
神经网络中参数的调试不包括 
A) 学习率α
B) 动量梯度下降的参数β
C) mini-Batch的大小
D) 输入图片大小
回答： 
答案： D


(3) [ 单选 ] ID:16609
Batch归一化的作用是 
A) 作用于输出层
B) 仅作用于输入层
C) 无法在隐藏层起作用
D) 用于输入层，甚至深度隐藏层的归一化过程
回答： 
答案： D


(4) [ 单选 ] ID:16610
Batch归一化即是 
A) 批量归一化
B) 仅对输入数据进行归一化
C) 仅对隐藏层进行归一化
D) 提升了参数搜索难度
回答： 
答案： A


(5) [ 单选 ] ID:16615
“熊猫方式”和鱼子酱方式的选择主要是通过什么决定的 
A) 计算机资源的充足与否
B) 测试集数量
C) 训练集数量
D) 隐藏层数量
回答： 
答案： A


(6) [ 单选 ] ID:16622
Batch归一化和神经网络中的什么功能类似 
A) relu
B) dropout
C) sigmoid
D) tanh
回答： 
答案： B


(7) [ 单选 ] ID:16616
Batch归一化步骤不包括 
A) 求每个训练批次数据的均值
B) 求每个训练批次数据的方差
C) 使用求得的均值和方差对该批次的训练数据做归一化，获得0-1分布
D) 求每个训练批次的和
回答： 
答案： D


(8) [ 单选 ] ID:16608
超参数范围中说法正确的是 
A) 随机取值可以提升搜索效率
B) 随机取值就是随机均匀取值
C) 范围就是[0,1]
D) 随机取值对搜索无影响
回答： 
答案： A


(9) [ 单选 ] ID:16613
实践中，Batch归一化通常和什么一起使用 
A) 训练集的mini-batch
B) 测试集的mini-batch
C) 整个训练集
D) 整个测试集
回答： 
答案： A


(10) [ 单选 ] ID:16621
Batch归一化在神经网络中的作用不包括 
A) 减少了隐藏值分布变化的数量
B) 减少了输入值改变的问题
C) 有轻微的正则化效果
D) 增加了输入值改变的问题
回答： 
答案： D


(11) [ 单选 ] ID:16612
对数坐标下，在[a,b]区间随机随机均匀地给r取值的说法错误的是 
A) 取最小值的对数得到a的值
B) 取最小值的对数得到b的值
C) 取最大值的对数得到b的值
D) 取任意值r，则超参数设置为10的r次方
回答： 
答案： B


(12) [ 单选 ] ID:16614
超参数范围中，随机取值指的是 
A) 随机选择标尺取值
B) 随机取值就是有效范围内随机均匀取值
C) 选择合适的标尺进行取值
D) 随机的进行均匀的取值
回答： 
答案： C


(13) [ 单选 ] ID:16607
神经网络中超参数的调试中最重要的参数是 
A) mini-Batch的大小
B) 动量梯度下降的参数β
C) 学习率α
D) 隐藏层数目
回答： 
答案： C


(14) [ 单选 ] ID:16611
Batch归一化的特点不包括 
A) 使参数搜索问题变得容易
B) 使神经网络对超参数的选择更加稳定
C) 超参数的范围更加庞大
D) 仅作用于输入层
回答： 
答案： D


(15) [ 单选 ] ID:16617
通过试验超参数的不同取值不可以 
A) 选择对训练集目标而言的最优解
B) 对于开发集而言的最优解
C) 超参搜索过程中最想优化的东西
D) 简化参数调试
回答： 
答案： D


(16) [ 单选 ] ID:16619
在典型的Batch归一化运用中需要用什么来估算 
A) 一个指数加权平均
B) 平均值
C) 方差
D) 最大值
回答： 
答案： A


(17) [ 单选 ] ID:16606
深度学习领域中，通常通过什么方式来选择参数 
A) 通过常识选择
B) 随机选择点来试验超参数效果
C) 选择输入的参数即可
D) 取离散参数的平均值
回答： 
答案： B


(18) [ 单选 ] ID:16620
Softmax的说法正确的是 
A) 主要用于二分类
B) 算出来为概率值
C) 将最小的概率值所对应的类别作为输入样本的输出类别
D) 所有的概率相加大于1
回答： 
答案： B


(19) [ 单选 ] ID:16605
早期的机器学习算法中，如果有两个超参数，通常通过什么方式来选择参数 
A) 网格中取样点，然后系统的研究这些数值
B) 比较参数的大小
C) 对参数进行迭代选择
D) 对参数进行平均
回答： 
答案： A


(20) [ 单选 ] ID:16604
Momentum(动量梯度下降法)的参数β一般默认值是 
A) 0.7
B) 0.8
C) 0.9
D) 1
回答： 
答案： C


(21) [ 多选 ] ID:16623
深度学习中通过动物来形容训练模型有 
A) 熊猫法
B) 鱼子酱法
C) 猫狗法
D) 大鱼法
回答： 
答案： AB


(22) [ 多选 ] ID:16632
Batch归一化步骤 
A) 求每个训练批次数据的均值
B) 求每个训练批次数据的方差
C) 使用求得的均值和方差对该批次的训练数据做归一化，获得0-1分布
D) 尺度变换和偏移
回答： 
答案： ABCD


(23) [ 多选 ] ID:16627
神经网络中超参数调试方法 
A) 随机选择点来试验超参数效果
B) 当你给超参数取值时，另一个惯例是采用由粗糙到精细的策略。
C) 只调试容易的
D) 给定默认值即可
回答： 
答案： AB


(24) [ 多选 ] ID:16625
对数坐标下，在[a,b]区间随机随机均匀地给r取值的说法正确的是 
A) 取最小值的对数得到a的值
B) 取最小值的对数得到b的值
C) 取最大值的对数得到a的值
D) 取最大值的对数得到b的值
回答： 
答案： AD


(25) [ 多选 ] ID:16626
神经网络中哪些参数需要调试 
A) 学习率α
B) 动量梯度下降的参数β
C) mini-Batch的大小
D) 隐藏层数目
回答： 
答案： ABCD


(26) [ 多选 ] ID:16628
Batch归一化的特点有 
A) 使参数搜索问题变得容易
B) 使神经网络对超参数的选择更加稳定
C) 超参数的范围更加庞大
D) 使训练更加容易
回答： 
答案： ABCD


(27) [ 多选 ] ID:16630
搜索参数的两种重要方式是 
A) 没有足够计算资源，通过每天观察，不断调整参数
B) 同时试验多种模型，获得学习曲线
C) 没有足够计算资源，通过试验多种模型，获得学习曲线
D) 拥有足够资源时，通过每天观察一个参数，来进行调整
回答： 
答案： AB


(28) [ 多选 ] ID:16631
关于“熊猫方式”和鱼子酱方式的选择说法正确的是 
A) 当计算机资源足够时选择“鱼子酱方式”
B) 当计算机资源足够时选择“熊猫方式”
C) 当计算机资源缺乏时选择“鱼子酱方式”
D) 当计算机资源缺乏时选择“熊猫方式”
回答： 
答案： AD


(29) [ 多选 ] ID:16624
通过试验超参数的不同取值可以 
A) 选择对训练集目标而言的最优解
B) 对于开发集而言的最优解
C) 超参搜索过程中最想优化的东西
D) 简化参数调试
回答： 
答案： ABC


(30) [ 多选 ] ID:16629
Batch归一化会起作用的原因包括 
A) 通过归一化所有的输入特征值，以获得类似范围的值，加速学习
B) 将参数归一化可以减缓学习速率
C) 可以使权重比你的网络更滞后或更深
D) 可以使权重比你的网络更超前或更深
回答： 
答案： AC


(31) [ 判断 ] ID:16636
Batch归一化增加了隐藏值分布变化的数量 
回答： 
答案： 否


(32) [ 判断 ] ID:16635
深度学习在不同的领域超参数的设定不能通用 
回答： 
答案： 否


(33) [ 判断 ] ID:16642
实践中，Batch归一化通常和训练集的mini-batch一起使用 
回答： 
答案： 是


(34) [ 判断 ] ID:16638
超参数范围中，随机取值就是有效范围内随机均匀取值 
回答： 
答案： 否


(35) [ 判断 ] ID:16637
Batch归一化不能加速训练 
回答： 
答案： 否


(36) [ 判断 ] ID:16640
若没有在超参数中作出正确的标尺决定，可以通过在均匀标出上选取多个数据进行弥补 
回答： 
答案： 是


(37) [ 判断 ] ID:16639
超参数范围中，随机取值是选择合适的标尺进行取值 
回答： 
答案： 是


(38) [ 判断 ] ID:16641
Batch归一化简化参数搜索问题 
回答： 
答案： 是


(39) [ 判断 ] ID:16634
超参数范围中，随机取值可以提升搜索效率 
回答： 
答案： 是


(40) [ 判断 ] ID:16633
神经网络中超参数的调试中最重要的参数是选择层数 
回答： 
答案： 否
