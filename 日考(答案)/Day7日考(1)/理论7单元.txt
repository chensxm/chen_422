(1) [ 单选 ] ID:16090
交叉熵作为( )可以衡量p与q的相似性。 
A) 损失函数
B) 激活函数
C) sigmoid函数
D) RELU函数
回答： 
答案： A


(2) [ 单选 ] ID:16102
常用的Sigmoid激活函数是一个在生物学中常见的（ ）函数 
A) X型
B) S型
C) L型
D) U型
回答： 
答案： B


(3) [ 单选 ] ID:15887
从( )、比较比对、逐渐逼近的循环过程就是建立数学模型 
A) 测量
B) 校准
C) 回归
D) 监督
回答： 
答案： A


(4) [ 单选 ] ID:16096
tf.multiply(x, y, name=None) ，其中类型跟张量x相同的张量是 
A) y
B) tf
C) name
D) None
回答： 
答案： A


(5) [ 单选 ] ID:16100
双曲正切函数tanh是，它总体上( ) S型函数 
A) 优于
B) 劣于
C) 等于
D) 大于等于
回答： 
答案： A


(6) [ 单选 ] ID:16103
在tensorflow下创建的会话tf.Session()，当上下文管理器退出时会话tf.Session()在关闭的同时且会( )释放相关资源 
A) 自动
B) 无法
C) 不清楚
D) 不能
回答： 
答案： A


(7) [ 单选 ] ID:16097
multiply这个函数实现的是元素级别的相乘，也就是两个相乘的数元素各自( ) 
A) 相与
B) 相乘
C) 相除
D) 相加
回答： 
答案： B


(8) [ 单选 ] ID:15886
学生数学建模比赛由中国工业与（ ）共同主办 
A) 应用数学学会
B) 教育部
C) 工信部
D) IEEE
回答： 
答案： A


(9) [ 单选 ] ID:16105
tf.subtract函数返回一个（ ），与 x 具有相同的类型 
A) Tensor
B) 数组
C) 矢量
D) 范数
回答： 
答案： A


(10) [ 单选 ] ID:16104
依据函数的曲线分布而命名的（ ）激活函数中，就是指Sigmoid函数 
A) U型
B) M型
C) X型
D) S型
回答： 
答案： D


(11) [ 单选 ] ID:16106
tf.nn.softmax_cross_entropy_with_logits函数是TensorFlow中常用的求( )的函数，即计算labels和logits之间的交叉熵（cross entropy） 
A) 信息熵
B) 信息元
C) logits
D) 交叉熵
回答： 
答案： D


(12) [ 单选 ] ID:16095
函数tf.constant（）不正确的用法是那一个选项? 
A) tensor=tf.constant(1)
B) tensor=tf.constant([1, 2])
C) tensor=tf.constant(-1, shape=[2, 3])
D) a = tf.constant([1.0, 2.0], name="a)
回答： 
答案： D


(13) [ 单选 ] ID:16092
用距离空间实现的分类问题，一般： 
A) 需要权重训练
B) 不需要权值训练
C) 不能分类
D) 需要选定特殊距离空间
回答： 
答案： B


(14) [ 单选 ] ID:16099
tf.reshape(tensor,shape,name=None)函数的作用是将tensor变换为参数shape形式，其中的shape为一个（ ）形式。-1所代表的含义是我们不用亲自去指定这一维的大小，函数会自动进行计算 
A) 矢量
B) 向量
C) 列表
D) 凸集
回答： 
答案： C


(15) [ 单选 ] ID:16088
( )函数就是softmax函数，是逻辑函数的一种推广 
A) 概率
B) 归一化
C) 损失函数
D) 空间
回答： 
答案： B


(16) [ 单选 ] ID:16093
一般来说，数学模型需要度量对比，则： 
A) 需要满足距离空间
B) 可以不满足距离空间
C) 与距离无关
D) 以上全对
回答： 
答案： A


(17) [ 单选 ] ID:16098
tf.matmul(a, b, transpose_a=False, transpose_b=False, adjoint_a=False, adjoint_b=False, a_is_sparse=False, b_is_sparse=False, name=None) 函数返回值是一个跟张量a和张量b类型一样的张量且最内部矩阵是a和b中的相应矩阵的（） 
A) 相与
B) 乘积
C) 相除
D) 相加
回答： 
答案： B


(18) [ 单选 ] ID:16091
交叉熵使用了( )梯度下降法 
A) sigmoid函数
B) RELU函数
C) tanh函数
D) cosh函数
回答： 
答案： A


(19) [ 单选 ] ID:16101
BP是back propagation的所写，是（ ）传播的意思 
A) 反向
B) 前向
C) 前后向
D) 全连接
回答： 
答案： A


(20) [ 单选 ] ID:16089
Cross Entropy交叉熵主要度量两个(  )分布间的差异性信息 
A) 概率
B) 矢量
C) 矩阵数据
D) 空间
回答： 
答案： A


(21) [ 多选 ] ID:16551
交叉熵函数是卷积神经网络中常使用函数softmax_cross_entropy的作用是? 
A) 在全连接层
B) 进行优化时用
C) 用信息熵
D) 用softmax映射到概率空间
回答： 
答案： ABCD


(22) [ 多选 ] ID:16501
有训练集包含num个训练样本，交叉熵在tf中的表述是： 
A) entropy = -tf.reduce_sum(y_actual*tf.log(y_predict))
B) entropy = -tf.reduce_mean(y_actual*tf.log(y_predict))
C) entropy = -tf.reduce_max(y_actual*tf.log(y_predict))
D) entropy = -tf.reduce_any(y_actual*tf.log(y_predict))
回答： 
答案： AB


(23) [ 多选 ] ID:16110
以下可以处理Mnist数据集的分类算法有 
A) KNN
B) 逻辑回归
C) 卷积神经网络
D) k-mean
回答： 
答案： ABC


(24) [ 多选 ] ID:16107
交叉熵为-1/m∑[y(i)log(hθ(x(i)))+(1?y(i))log(1?hθ(x(i)))] 
A) m是训练输入元素个数
B) ∑的加和长度是输入个数
C) y（i）来自标签的onehot表
D) hθ(x(i))和标签one-hot表维度一致
回答： 
答案： ABCD


(25) [ 多选 ] ID:15858
人工智能被分为三大学派或者三大主义，请从以下选项中选择出( )? 
A) symbolicism
B) connectionism
C) actionism
D) 机器学习
回答： 
答案： ABC


(26) [ 多选 ] ID:16112
在tensorflow中，tf.reshape函数的参数是(tensor,shape,name=None)，以下哪些描述是正确的? 
A) 函数的作用是将tensor变换为参数shape形式
B) 其中的shape为一个列表形式
C) name可省略
D) -1所代表的含义是我们不用亲自去指定这一维的大小，函数会自动进行计算
回答： 
答案： ABCD


(27) [ 多选 ] ID:16111
反向传播算法的过程如下: 
A) 初始化联结权重Wij，对于输入的训练样本，求取每个节点输出和最终输出层的输出值
B) 对输出层求取偏导数
C) 对于隐藏层求取偏导数
D) 求取输出误差对于每个权重的梯度，更新权重
回答： 
答案： ABCD


(28) [ 多选 ] ID:16550
tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)意义： 
A) 定义学习步长
B) 优化器
C) 交叉熵损失函数
D) 开始训练
回答： 
答案： ABC


(29) [ 多选 ] ID:16108
分类任务的损失函数，可以是： 
A) 信息熵
B) 最小二乘平方损失
C) 距离模型
D) 概率模型
回答： 
答案： ABCD


(30) [ 多选 ] ID:16109
对于tf.nn.SoftMax函数，它可以： 
A) 用于多类别分类
B) 映射到概率空间
C) 压缩数据
D) 用于卷积层
回答： 
答案： ABC


(31) [ 判断 ] ID:16123
SOFTMAX函数，是用来将全数域函数结果映射到概率空间。 
回答： 
答案： 是


(32) [ 判断 ] ID:16120
给定概率空间，信息熵是针对有限个概率事件的。 
回答： 
答案： 是


(33) [ 判断 ] ID:16115
print(sess.run(tf.sigmoid(a)))是逻辑回归 
回答： 
答案： 否


(34) [ 判断 ] ID:16114
如果想获取当前场景背景的图像，则可以通过图像差分便可以提取出前景 
回答： 
答案： 是


(35) [ 判断 ] ID:16118
损失函数中，基于信息熵的损失函数，与平方范数损失函数互有矛盾。 
回答： 
答案： 否


(36) [ 判断 ] ID:16121
信息熵是针对有限个概率事件的。 
回答： 
答案： 是


(37) [ 判断 ] ID:16119
在机器学习中，采用信息熵可以解决决策树问题。 
回答： 
答案： 是


(38) [ 判断 ] ID:16085
交叉熵（cross entropy）描述的是两个概率分布之间的距离，距离越小表示这两个概率越相近，越大表示两个概率差异越大。 
回答： 
答案： 是


(39) [ 判断 ] ID:16117
Tensorflow中SOFTMAX函数是针对多路输入的概率化输出的。 
回答： 
答案： 是


(40) [ 判断 ] ID:16122
对于一个给定的概率空间，信息熵会有多个。 
回答： 
答案： 否


(41) [ 判断 ] ID:16116
tf.matmul()将矩阵a乘以矩阵b，生成a*b 
回答： 
答案： 是


(42) [ 判断 ] ID:16113
形状相似的两个信号序列，其相关系数也大。 
回答： 
答案： 是
