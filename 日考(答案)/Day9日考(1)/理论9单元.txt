(1) [ 单选 ] ID:16168
mini-batch下降的效果是： 
A) 损失函数值一直下降
B) 损失函数值总体趋势下降
C) 比梯度下降速度快
D) 梯度下降不明显
回答： 
答案： B


(2) [ 单选 ] ID:16169
不是随机梯度下降的特点是： 
A) 批量数值选取为1
B) 学习率逐渐减小
C) 可以达到最小值
D) 在最小值附近波动
回答： 
答案： C


(3) [ 单选 ] ID:16173
局部平均值又被称为 
A) 常规平均值
B) 栅格平均值
C) 移动平均值
D) 唯一平均值
回答： 
答案： C


(4) [ 单选 ] ID:16174
指数加权平均数公式的好处在于 
A) 只需要少量参数
B) 占用极少内存
C) 不适用学习率
D) 是非监督式学习
回答： 
答案： B


(5) [ 单选 ] ID:16175
动量梯度算法，是通过学习率和（）控制梯度下降的 
A) 指数加权平均数
B) 局部平均值
C) 全局平局值
D) 方差
回答： 
答案： A


(6) [ 单选 ] ID:16166
mini-batch的原理是 
A) 选取数据中部分数据进行梯度下降
B) 和批量梯度下降相同，只是将算法进行优化
C) 将数据每次进行一小批次处理，通过迭代将数据全部处理
D) 随机选取一些数据，计算梯度进行下降，每次将学习率降低一点
回答： 
答案： C


(7) [ 单选 ] ID:16183
在优化算法中，学习率会： 
A) 保持不变
B) 持续减小
C) 持续增大
D) 不变
回答： 
答案： B


(8) [ 单选 ] ID:16176
RMSprop算法的特点是 
A) 指数加权平均数求和
B) 指数加权平均数先平方再开方
C) 指数加权平均数求微分
D) 指数加权平均数求均方误差
回答： 
答案： B


(9) [ 单选 ] ID:16171
如果数据集大小为m，mini-batch的大小为 
A) 1
B) m
C) 大于m
D) 大于2，小于m
回答： 
答案： D


(10) [ 单选 ] ID:16165
关于mini-batch说法错误的是 
A) 指的是批量梯度下降
B) 适用于样本量小的数据集
C) 每一次只运算部分数据，最后将全部数据进行运算
D) 适用于样本量大的数据
回答： 
答案： B


(11) [ 单选 ] ID:16177
Adam算法的核心是 
A) 强化了RMSprop算法
B) 强化了动量梯度算法
C) 同时使用Momentum和RMSprop算法
D) 没有核心
回答： 
答案： C


(12) [ 单选 ] ID:16179
对于批量梯度下降，使用优化算法是为了在迭代过程中（）： 
A) 增大损失值
B) 使学习率衰减
C) 降低损失值
D) 提升正则项范围
回答： 
答案： B


(13) [ 单选 ] ID:16182
Adam算法的tensorflow代码是： 
A) tf.example.AdamOptimizer
B) tf.train.AdamOptimizer
C) tf.nn.AdamOptimizer
D) tf.AdamOptimizer
回答： 
答案： B


(14) [ 单选 ] ID:16172
加权平均值是通过（）得到的 
A) 局部平均值
B) 局部方差
C) 全局平均值
D) 全局方差
回答： 
答案： A


(15) [ 单选 ] ID:16170
优化算法计算用到了： 
A) 标准差加权平均
B) 方差加权平均
C) 对数加权平均
D) 指数加权平均
回答： 
答案： D


(16) [ 单选 ] ID:16178
RMSprop相比Momentum，可以选择更大的（） 
A) 损失函数
B) 学习率
C) 激活函数
D) 样本集
回答： 
答案： B


(17) [ 单选 ] ID:16181
要想让损失值最小，需要找到（） 
A) 鞍点
B) 局部最优解
C) 转折点
D) 全局最优解
回答： 
答案： D


(18) [ 单选 ] ID:16164
以下四个mini-batch选取样本数量，哪一个效果最好 
A) 58
B) 60
C) 62
D) 64
回答： 
答案： D


(19) [ 单选 ] ID:16167
mini-batch指的是 
A) 小批量梯度下降
B) 随机梯度下降
C) 批量梯度下降
D) 小批量损失计算
回答： 
答案： A


(20) [ 单选 ] ID:16180
优化算法减小学习率的原因是： 
A) 一种默认定式
B) 减少内存容量
C) 避免在最优解附近大幅度摆动
D) 减少迭代次数
回答： 
答案： C


(21) [ 多选 ] ID:16192
mini-batch比较好的取值有 
A) 16
B) 32
C) 64
D) 128
回答： 
答案： ABCD


(22) [ 多选 ] ID:16184
以下属于梯度下降的有 
A) BGD
B) SGD
C) Mini-Batch
D) dropout
回答： 
答案： ABC


(23) [ 多选 ] ID:16188
Adam算法结合了（）算法于一身 
A) RMSprop算法
B) 均方误差
C) Momentum算法
D) 交叉熵
回答： 
答案： AC


(24) [ 多选 ] ID:16189
梯度为0的点可以是 
A) 局部最优解
B) 全局最优解
C) 鞍点
D) 转折点
回答： 
答案： ABC


(25) [ 多选 ] ID:16190
能够跳出局部最优解的算法有： 
A) Adam
B) Momentum
C) RMSprop
D) Lasso
回答： 
答案： ABC


(26) [ 多选 ] ID:16193
Momentum算法可以使用（）下降 
A) batch
B) mini-batch
C) Lasso
D) Ridge
回答： 
答案： AB


(27) [ 多选 ] ID:16186
常用的梯度优化算法有： 
A) dropout
B) 动量梯度下降法
C) RMSprop
D) Adam
回答： 
答案： BCD


(28) [ 多选 ] ID:16191
Momentum和RMSprop算法的特点是： 
A) RMSprop可以取更大学习率
B) RMSprop是将指数平均值平方再开发
C) RMSprop下降更好消除摆动
D) RMSprop和Momentum都可以优化算法
回答： 
答案： ABCD


(29) [ 多选 ] ID:16185
随机梯度下降的特点是： 
A) mini-batch大小为1
B) 每次迭代有可能远离最优解
C) 永远不会收敛
D) 不能使用向量化加速
回答： 
答案： ABCD


(30) [ 多选 ] ID:16187
动量梯度下降算法是通过（）和（）控制梯度下降的 
A) 平均值
B) 指数加权平均值
C) 学习率
D) 方差
回答： 
答案： BC


(31) [ 判断 ] ID:16202
鞍点是多维空间中函数的最优解 
回答： 
答案： 否


(32) [ 判断 ] ID:16199
随机梯度下降的学习率不会改变 
回答： 
答案： 否


(33) [ 判断 ] ID:16194
RMSprop算法是将指数加权平均数先平方再开方 
回答： 
答案： 是


(34) [ 判断 ] ID:16203
全局最优解的函数损失值最小 
回答： 
答案： 是


(35) [ 判断 ] ID:16198
mini-batch是指只处理部分数据的梯度下降算法 
回答： 
答案： 否


(36) [ 判断 ] ID:16201
mini-batch不能使用正则化 
回答： 
答案： 否


(37) [ 判断 ] ID:16195
优化算法减小学习率是为了减少迭代次数 
回答： 
答案： 否


(38) [ 判断 ] ID:16197
训练集分割为小一点的子集训练，这些子集被取名为 mini-batch 
回答： 
答案： 是


(39) [ 判断 ] ID:16200
动量梯度下降法运行速度几乎总是快于标
准的梯度下降算法 
回答： 
答案： 是


(40) [ 判断 ] ID:16196
mini-batch选取样本数量尽量选取2的次方 
回答： 
答案： 是
