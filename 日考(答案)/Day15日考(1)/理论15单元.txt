(1) [ 单选 ] ID:16350
将一个骰子的“2”修改成“1”，那么掷这个骰子得信息熵会。 
A) 增大
B) 减少
C) 不变
D) 不确定
回答： 
答案： B


(2) [ 单选 ] ID:16651
t = tf.Variable([1,2,3]),  tf.multiply(t, 2)的结果是 
A) [1, 2, 3]
B) [2, 3, 4]
C) [2, 4, 6]
D) [2, 4, 3]
回答： 
答案： C


(3) [ 单选 ] ID:16351
如果一个硬币，一次算一个事件结果；两次算一个事件结果；他们是 
A) 一个概率空间
B) 两个概率空间
C) 一个样本空间
D) 一个概率空间，两个事件
回答： 
答案： B


(4) [ 单选 ] ID:16254
能消除过拟合的方法是 
A) 线性化
B) 非线性化
C) 归一化
D) 正则化
回答： 
答案： D


(5) [ 单选 ] ID:16362
tf.truncated_normal(shape, mean, stddev) :shape表示生成张量的维度，mean是均值，stddev是标准差。这个函数产生（ ）分布，均值和标准差自己设定 
A) 凸集
B) 凹集
C) 正态
D) 负态
回答： 
答案： C


(6) [ 单选 ] ID:16357
贝努力实验的基础概率空间是： 
A) 均匀分布
B) 高斯分布
C) 0-1分布
D) 指数分布
回答： 
答案： C


(7) [ 单选 ] ID:16353
对于序列[1,2.1,1.9,1,3.1,2.9]可能是 
A) 二项式分布
B) 高斯分布
C) 均匀分布
D) 0-1分布
回答： 
答案： C


(8) [ 单选 ] ID:16361
为了屏蔽一些不必要的警告和错误，常使用编程语句例如os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'主要作用是屏蔽什么? 
A) 警告
B) 错误
C) 信息和警告
D) 反汇编过程
回答： 
答案： C


(9) [ 单选 ] ID:16352
tf.slice函数作用是：从tf的一个张量中， 
A) 取出部分张量数据
B) 取出矩阵
C) 取出数
D) 取出向量
回答： 
答案： A


(10) [ 单选 ] ID:16349
sin(sinx)的导数是 
A) cosx(cos(sinx))
B) cos(cosx)
C) cos(sinx)
D) sin(cosx)
回答： 
答案： A


(11) [ 单选 ] ID:16356
将一个骰子的“3”和“2”修改成“1”，那么掷这个骰子得信息熵会： 
A) 增大
B) 减少
C) 不变
D) 不确定
回答： 
答案： B


(12) [ 单选 ] ID:16599
深度卷积神经网络中，（）提取的特征往往是比较简单的（如检测点、线、亮度）？ 
A) 深层
B) 浅层
C) 最后几层
D) 所有层
回答： 
答案： B


(13) [ 单选 ] ID:16643
x的值是False，tf.cast(x, tf.float32)的结果是 
A) 0.0
B) False
C) 1.0
D) True
回答： 
答案： A


(14) [ 单选 ] ID:16359
神经风格迁移是指将（ ）图像的风格应用于目标图像，同时保留目标图像的内容。 
A) 参考
B) 卷积
C) 池化
D) 腐蚀
回答： 
答案： A


(15) [ 单选 ] ID:16645
x = tf.constant([[1., 1.], [2., 2.]])，tf.reduce_mean(x)的结果是 
A) 1.0
B) 2.0
C) 1.5
D) 2.5
回答： 
答案： C


(16) [ 单选 ] ID:16360
风格迁移这一想法与( )生成的想法密切相关 
A) 黑帽
B) 纹理
C) 池化
D) 开运算
回答： 
答案： B


(17) [ 单选 ] ID:16355
复合函数sin(sinx)的导数是 
A) cosx*(cos(sinx))
B) cos(cosx)
C) cos(sinx)
D) sin(cosx)
回答： 
答案： A


(18) [ 单选 ] ID:16354
样本点数量相同的概率空间，可以等价互换。 
A) 是
B) 不能
C) 部分能
D) 部分不能
回答： 
答案： A


(19) [ 单选 ] ID:16258
最大池化max_pool有几个参数，如value, ksize参数, strides参数, padding参数, name=None参数，用于池化窗口的参量是那一个? 
A) value
B) ksize
C) strides
D) padding
回答： 
答案： B


(20) [ 单选 ] ID:16358
对一个概率空间，进行多种划分，其信息熵是 
A) 一定相等
B) 一定不等
C) 不确定无关联
D) 互有关联
回答： 
答案： C


(21) [ 多选 ] ID:16367
通常所用的函数，可以描述为? 
A) 一个集合
B) 一个映射
C) 一个概率空间
D) 一个线性空间
回答： 
答案： AB


(22) [ 多选 ] ID:16403
在一个概率空间，经过不同划分后， 
A) 信息熵不同
B) 构成新概率空间
C) 信息熵不变
D) 不可能有多种划分
回答： 
答案： AB


(23) [ 多选 ] ID:16601
神经风格迁移的损失函数包含哪几部分？ 
A) 内容图像的损失函数
B) 风格图像的损失函数
C) 交叉熵损失函数
D) 均方差损失函数
回答： 
答案： AB


(24) [ 多选 ] ID:16269
卷积后图像尺寸不变化，对于Padding的描述，正确的是 
A) 以0填充
B) 填充厚度是卷积核厚度减1的一半
C) 步长为1
D) 图像尺寸变小
回答： 
答案： AB


(25) [ 多选 ] ID:16364
tf.slice( image,input,size )从，image取出局部 
A) image是输入张量
B) input是定位点
C) size是取出数据描述
D) image只能有四维张量
回答： 
答案： ABC


(26) [ 多选 ] ID:16368
信息熵是对概率空间的整个系统描述，这里的概率空间描述正确的是： 
A) 包含全部样本点
B) 可能有多种划分
C) 是核函数
D) 解决线性分类问题
回答： 
答案： AB


(27) [ 多选 ] ID:16366
神经网络解决非线性分类，是通过： 
A) 构造神经网络
B) 用激活函数
C) 训练权重矩阵
D) 让损失最大化
回答： 
答案： ABC


(28) [ 多选 ] ID:16370
神经风格迁移过程包括? 
A) 创建网络
B) 损失函数最小化
C) 梯度下降过程LOSS最小化
D) 数据清洗
回答： 
答案： ABC


(29) [ 多选 ] ID:16369
所谓函数的另一种描述是： 
A) 它是一个集合
B) 可以看成一个映射
C) 一个概率空间
D) 一个线性空间
回答： 
答案： AB


(30) [ 多选 ] ID:16365
读入的图片，要想用tf卷积，需要 
A) 将图片转换成张量
B) 需要按照tf的四元组格式
C) 需要定义卷积核
D) 定义步长
回答： 
答案： ABCD


(31) [ 判断 ] ID:16381
使用keras可以实现神经风格迁移 
回答： 
答案： 是


(32) [ 判断 ] ID:16374
对于概率空间一个任意划分，其结果依然是概率空间。 
回答： 
答案： 是


(33) [ 判断 ] ID:16375
常见编码中，ASC和Unicode的bit位相同。 
回答： 
答案： 否


(34) [ 判断 ] ID:16376
在古典概率空间，其样本点对应概率必须相等。 
回答： 
答案： 是


(35) [ 判断 ] ID:16384
风格迁移可以被卷积神经网络不同激活的内部相互关系所捕捉到。 
回答： 
答案： 是


(36) [ 判断 ] ID:16382
风格迁移是指创建一张新图像，保留目标图像的内容的同是还抓住了参考图像的风格 
回答： 
答案： 是


(37) [ 判断 ] ID:16385
人脸识别数据集通常是2分类图片 
回答： 
答案： 是


(38) [ 判断 ] ID:16371
对于一个固定划分，信息熵是针对全体概率事件的。 
回答： 
答案： 是


(39) [ 判断 ] ID:16380
神经风格迁移可以用任何预训练卷积神经网络来实现 
回答： 
答案： 是


(40) [ 判断 ] ID:16383
风格迁移的内容可以被卷积神经网络更靠顶部的层激活所捕捉到。 
回答： 
答案： 是


(41) [ 判断 ] ID:16379
古典概率通常又叫事前概率，是指当随机事件中各种可能发生的结果及其出现的次数都可以由演绎或外推法得知，而无需经过任何统计试验即可计算各种可能发生结果的概率。 
回答： 
答案： 是


(42) [ 判断 ] ID:16378
通过池化操作，必须保证特征损失不能太大。 
回答： 
答案： 是


(43) [ 判断 ] ID:16377
随机生成的卷积核，个别一些卷积核对特征提取几乎无贡献，但不影响运算。 
回答： 
答案： 是


(44) [ 判断 ] ID:16373
通过池化操作，特征损失不能太大。 
回答： 
答案： 是


(45) [ 判断 ] ID:16372
概率空间的样本点，其对应概率必须相等。 
回答： 
答案： 是
