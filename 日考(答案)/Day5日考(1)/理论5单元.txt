(1) [ 单选 ] ID:15987
tf的全局变量初始化语句是 
A) tf.global_variables_initializer()
B) tf.variables_initializer
C) sess.run( w.initializer )
D) tf.initializer()
回答： 
答案： A


(2) [ 单选 ] ID:15997
BP算法是通过梯度下降的方法对联结权重进行优化，即需要计算误差函数对联结权重的( )。 
A) 导数
B) 偏导数
C) 平方差
D) 标准偏差
回答： 
答案： B


(3) [ 单选 ] ID:15983
tf语句y = tf.multiply(a, b) 的意思是 
A) 构造一个变量
B) 构造一个op（操作）
C) 构造一个任务
D) 构造一个函数
回答： 
答案： B


(4) [ 单选 ] ID:15993
编程中，通常要进行全局变量初始化操作，可以使用下列那一个语句（） 
A) init_op = tf.global_variables_initializer()
B) init_op = tf.variables_initializer()
C) init_op = tf.initializer()
D) init_op = np.global_variables_initializer()
回答： 
答案： A


(5) [ 单选 ] ID:15996
神经网络中使用激活函数处理非线性因素，是因为( )的效果不够 
A) 非线性模型
B) 非线性+线性模型
C) 双曲线性模型
D) 线性模型
回答： 
答案： D


(6) [ 单选 ] ID:15988
多层前馈网络也称为(  ) 
A) BP网络
B) 隐藏层网络
C) 输入层网络
D) 输出层网络
回答： 
答案： A


(7) [ 单选 ] ID:15984
tf定义一个占位符号的语句是 
A) Y = tf.zeros(2.0,shape=[1,2])
B) X = tf.variable(2.0,shape=[1,2])
C) Y = tf.placeholder(tf.float32)
D) Y=ones(2.0,shape=[1,2])
回答： 
答案： C


(8) [ 单选 ] ID:15989
前向传播过程+反向传播过程 ，接下来只需要进行（ ），不断调整边的权重 
A) 求偏导
B) 求和
C) 四舍五入
D) 迭代
回答： 
答案： D


(9) [ 单选 ] ID:15985
在实现tf变量A和B按照对应位相乘的函数是（） 
A) tf.multiply
B) tf.matmul
C) A*B
D) A.puls(B)
回答： 
答案： A


(10) [ 单选 ] ID:15991
初始化时通常使用（ ）随机数给各权值和阈值赋初值。 
A) 偏导数
B) 大的
C) 小的
D) 均方根
回答： 
答案： C


(11) [ 单选 ] ID:15995
激活函数把 “ ( ) ” 保留并映射出来 
A) 半激活的神经元的特征
B) 未激活的神经元的特征
C) 激活的神经元的特征
D) 无所谓状态的神经元的特征
回答： 
答案： C


(12) [ 单选 ] ID:15981
tf定义一个标量变量X，其语句是 
A) X = tf. variable (2.0,shape=[1,2])
B) X = tf.Variable (2.0,dtype=tf.float32)
C) X = tf. variable (2.0,shape=[1,2])
D) X = tf. constant (2.0,btype=tf.float32 )
回答： 
答案： B


(13) [ 单选 ] ID:15992
启动图/会话的第一步是创建一个Session对象，如： 
A) sess = tf.Session()
B) sess.close()
C) tf.add
D) tf.eqeal
回答： 
答案： A


(14) [ 单选 ] ID:15990
1985年Rumelhart等人发展了（ ）理论 
A) BP算法
B) 求偏导
C) 激活函数
D) 前向算法
回答： 
答案： A


(15) [ 单选 ] ID:15986
在tf中将变量A的值赋给B的语句是（） 
A) tf.assign
B) tf.sub
C) tf.add
D) tf.eqeal
回答： 
答案： A


(16) [ 单选 ] ID:15994
在tf函数中，通常需将一个变量的值赋给另一个变量，可以使用编程语句是（） 
A) tf.assign（）
B) tf.sub（）
C) tf.add（）
D) tf.eqeal（）
回答： 
答案： A


(17) [ 单选 ] ID:15980
alf属于[0,1]，A，B是平面上点，alf*A + （1-alf）*B的意义是 
A) 过AB两点直线
B) A和B构成的线段
C) A和B的向量和
D) A和B构成的集合
回答： 
答案： B


(18) [ 单选 ] ID:15999
在tensorflow中，能实现2个向量代数运算的函数是？ 
A) c = tf.greater( a,b )
B) a = tf.subtract( a,b )
C) b = tf.Equal( a,b )
D) tf.constant
回答： 
答案： B


(19) [ 单选 ] ID:15982
如需要定义tf的常量X = tf.constant(2.0,shape=[1,2])语句结果是 
A) 2
B) [2.0, 2.0]
C) [[2.0, 2.0]]
D) [[2.0], [2.0]]
回答： 
答案： C


(20) [ 单选 ] ID:15998
缩写Artificial Intelligence 是指 
A) AO
B) AI
C) AL
D) AN
回答： 
答案： B


(21) [ 多选 ] ID:16001
在tf语句中，a= tf.Variable(25）；执行sess.run(a)语句以后，以下说法正确的是： 
A) 执行一个操作节点
B) 从python操作tf模块
C) 执行tf内的python模块
D) 实现python引用tf源代码
回答： 
答案： AB


(22) [ 多选 ] ID:16008
在AI领域，图像识别技术经常会应用到好多场景，比如应用案例有? 
A) 人脸检测
B) 表情判断
C) 动作识别
D) 无人驾驶、车牌识别
回答： 
答案： ABCD


(23) [ 多选 ] ID:16000
以下说法，关于tensorflow描述正确的是 
A) 集成了主流的机器学习算法
B) 支持分布式部署
C) 支持Python语言开发
D) 不支持深度学习
回答： 
答案： ABC


(24) [ 多选 ] ID:16002
tf中能实现两个向量代数运算的语句是 
A) c = tf.greater( a,b )
B) a = tf.subtract( a,b )
C) b = tf.Equal( a,b )
D) d = tf.matmul( a,b )
回答： 
答案： BD


(25) [ 多选 ] ID:16004
语句tf.transpose(x).eval()的意义是： 
A) 对x进行转置
B) 从tf环境中获取x的数据交给python环境
C) 不对x转置
D) 不能获知x的内容
回答： 
答案： AB


(26) [ 多选 ] ID:16006
BP算法是由( )、( )两个过程完成 
A) 正向传播
B) 反向传播
C) 求偏导
D) 激活函数
回答： 
答案： AB


(27) [ 多选 ] ID:16003
在tf中，与全局变量初始化相关的操作语句是 
A) init_op = tf.global_variables_initializer()
B) with tf.Session() as sess:
C) sess.run(init_op)
D) import numpy as np
回答： 
答案： ABC


(28) [ 多选 ] ID:16005
关于反向传播算法，它也存在不足，其主要有： 
A) 训练时间较长
B) 完全不能训练，训练时由于权值调整过大使激活函数达到饱和
C) 易陷入局部极小值
D) “喜新厌旧”。训练过程中，学习新样本时有遗忘旧样本的趋势
回答： 
答案： ABCD


(29) [ 多选 ] ID:16007
反向算法BP网络学习包括以下（ ）个过程 
A) 组成输入模式由输入层经过隐含层向输出层的“模式顺传播”过程
B) 网络的期望输出与实际输出之差的误差信号由输出层经过隐含层逐层修正连接权的“误差逆传播”过程
C) 由“模式顺传播”与“误差逆传播”的反复进行的网络“记忆训练”过程
D) 网络趋向收敛即网络的总体误差趋向极小值的“学习收敛”过程
回答： 
答案： ABCD


(30) [ 多选 ] ID:16009
Anaconda包括( )以及一大堆安装好的工具包，比如：numpy、pandas等 
A) Conda
B) Python
C) OPENCV
D) TPU
回答： 
答案： AB


(31) [ 判断 ] ID:16014
TensorFlow最初由Google大脑的研究员和工程师开发出来，用于机器学习和神经网络方面的研究，于2015.10宣布开源，在众多深度学习框架中脱颖而出，在Github上获得了最多的Star量 
回答： 
答案： 是


(32) [ 判断 ] ID:16018
前向传播算法的作用是计算输入层结点对隐藏层结点的影响 
回答： 
答案： 是


(33) [ 判断 ] ID:16021
有一个隐藏层网络，就是二层神经网络 
回答： 
答案： 是


(34) [ 判断 ] ID:16019
import tensorflow as tf
print( help(tf))  这个语句是寻求帮助的过程 
回答： 
答案： 是


(35) [ 判断 ] ID:16010
全局变量初始化语句后，变量和常量都被初始化。 
回答： 
答案： 否


(36) [ 判断 ] ID:16020
BP算法可以使网络权值收敛到一个最终解，但它并不能保证所求为误差超平面的全局最优解，也可能是一个局部极小值。这主要是因为BP算法所采用的是梯度下降法 
回答： 
答案： 是


(37) [ 判断 ] ID:16017
BP算法是通过梯度下降法对联结权重进行优化，所以需要计算误差函数对联结权重的偏导数。 
回答： 
答案： 是


(38) [ 判断 ] ID:16022
tf.set_random_seed 函数可以从两个seed中获得依赖随机seed的操作，图形级seed和操作级seed 
回答： 
答案： 是


(39) [ 判断 ] ID:16011
占位符的定义，有助于将预先不知道的值带入运算中。 
回答： 
答案： 是


(40) [ 判断 ] ID:16015
有了联结权重w和激活函数H(x)之后，就可以由前往后计算，依次算出所有的经过联结权重处理后的输出值，经过激活函数处理之后的输出值，最终算出输出层的y值。这就是前向传播算法 
回答： 
答案： 是


(41) [ 判断 ] ID:16016
经过权值和阈值不断迭代调整的过程，就是网络的学习与训练过程 
回答： 
答案： 是


(42) [ 判断 ] ID:16023
BP传播算法过程是经过信号前向传播与误差反向传播，W权值和阈值的调整反复进行，一直进行到预先设定的学习训练次数，或输出误差减小到允许的程度 
回答： 
答案： 是


(43) [ 判断 ] ID:16012
变量初始化有三种方式，个别，部分，全局变量初始化。 
回答： 
答案： 是


(44) [ 判断 ] ID:16013
TensorFlow基于数据流图，用于大规模分布式数值计算的开源框架。节点表示某种抽象的计算，边表示节点之间相互联系的张量 
回答： 
答案： 是
