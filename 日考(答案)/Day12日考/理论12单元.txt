(1) [ 单选 ] ID:16244
非常经典的LeNet-5神经网络其FC层处理完成以后，输出的结果会再经过那一个激活函数输出（）? 
A) RelU
B) sigmoid
C) tanh
D) sin
回答： 
答案： B


(2) [ 单选 ] ID:16241
研究发现 Inception-ResNet 模型可以在更少的 epoch 内达到更( )的准确率 
A) 低
B) 高
C) 先高后低
D) 不确定
回答： 
答案： B


(3) [ 单选 ] ID:16238
与 Inception 同年提出的优秀网络还有（ ），它相比于 AlexNet 有更小的卷积核和更深的层级 
A) VGG-Net
B) Inception
C) ResNet
D) LeNet-5
回答： 
答案： A


(4) [ 单选 ] ID:16257
使用二维滤波器滑动到被卷积的二维图像上所有位置，并在每个位置上与该像素点及其领域像素点做内积,这就是（ ） 
A) 一维卷积
B) 二维卷积
C) 三维卷积
D) 四维卷积
回答： 
答案： B


(5) [ 单选 ] ID:16236
从( )到AlexNet。进化之路一：网络结构加深；进化之路二：加强卷积功能；进化之路三：从分类到检测；进化之路四：新增功能模块。 
A) LeNet
B) AlexNet
C) VGG
D) ResNet
回答： 
答案： A


(6) [ 单选 ] ID:16248
AlexNet网络结构有8层，其中有5个卷积层和（ ）个全连接层 
A) 3
B) 5
C) 16
D) 19
回答： 
答案： A


(7) [ 单选 ] ID:16251
LeNet-5网络是针对灰度图进行训练的，输入图像大小为32*32*1，不包含输入层的情况下共有（ ）层，即2层卷积，2层池化，3层全连接，每一层都包含可训练参数（连接权重） 
A) 5
B) 7
C) 8
D) 16
回答： 
答案： B


(8) [ 单选 ] ID:16256
能消除overfitting过拟合的方法是那一个? 
A) 线性化
B) 非线性化
C) 归一化
D) 定义dropout
回答： 
答案： D


(9) [ 单选 ] ID:16252
神经网络LenNet-5共有7层（不包括输入层）,主要有2个( )、2个下抽样层（池化层）、3个全连接层3种连接方式 
A) 输入层
B) 卷积层
C) 池化层
D) FC层
回答： 
答案： B


(10) [ 单选 ] ID:16242
Inception v1的亮点之一卷积层共有的一个功能，可以实现通道方向的降维和增维，至于是降还是增，取决于卷积层的通道数（滤波器个数），在Inception v1中( )卷积用于降维，减少weights大小和feature map维度。 
A) 1*1
B) 2*2
C) 3*3
D) 5*5
回答： 
答案： A


(11) [ 单选 ] ID:16246
GoogLeNet 层数很深，是( ) 层网络，为了避免上述提到的梯度消失问题，googlenet巧妙的在不同深度处增加了两个loss来保证梯度回传消失的现象。 
A) 152
B) 22
C) 8
D) 5
回答： 
答案： B


(12) [ 单选 ] ID:16250
LeNet-5神经网络是Yann LeCun在1998年设计的用于手写数字识别的卷积神经网络，当年美国大多数银行就是用它来识别（ ）上面的手写数字的，它是早期卷积神经网络中最有代表性的实验系统之一 
A) 支票
B) 钱币
C) 人脸
D) 汽车
回答： 
答案： A


(13) [ 单选 ] ID:16239
Inception是一个( )层的深度网络，它的质量是在分类和检测领域进行了评估 
A) 5
B) 8
C) 22
D) 152
回答： 
答案： C


(14) [ 单选 ] ID:16253
Inception网络中，一个 5×5 的卷积在计算成本上是一个 3×3 卷积的 ( ) 倍。所以叠加两个 3×3 卷积实际上在性能上会有所提升 
A) 2.78
B) 2.87
C) 7.28
D) 8.72
回答： 
答案： A


(15) [ 单选 ] ID:16247
经典的网络LeNet-5第七层是全连接层Output .这个层共有10个节点分别代表输出数字范围是( ) 
A) 0和9
B) 0*9
C) 0到9
D) 0到10
回答： 
答案： C


(16) [ 单选 ] ID:16237
Alex在2012年提出的( )网络结构模型引爆了神经网络的应用热潮，并赢得了2012届图像识别大赛的冠军，使得CNN成为在图像分类上的核心算法模型。AlexNet 该模型一共分为八层，5个卷积层,，以及3个全连接层 
A) LeNet
B) AlexNet
C) VGG
D) ResNet
回答： 
答案： B


(17) [ 单选 ] ID:16243
Inception v2的亮点之一加入了BN层，减少了InternalCovariate Shift（内部neuron的数据分布发生变化），使每一层的输出都规范化到一个N(0, 1)的高斯，从而增加了模型的（ ），可以以更大的学习速率训练，收敛更快，初始化操作更加随意，同时作为一种正则化技术，可以减少dropout层的使用。 
A) 范数
B) 准确度
C) 过拟和
D) 鲁棒性
回答： 
答案： D


(18) [ 单选 ] ID:16245
设计为8层的卷积神经网络AlexNet网络成功使用( )函数，其效果远远地超过了Sigmoid函数 
A) RelU函数
B) sigmoid函数
C) tanh函数
D) sin函数
回答： 
答案： A


(19) [ 单选 ] ID:16249
下列选项中，（ ） 模型一共分为八层，5个卷积层,3个全连接层 
A) LeNet
B) AlexNet
C) VGG
D) ResNet
回答： 
答案： B


(20) [ 单选 ] ID:16240
Inception V2网络中,一个 3×3 的卷积等价于首先执行一个 1×3 的卷积再执行一个( )的卷积。他们还发现这种方法在成本上要比单个 3×3 的卷积降低 33% 
A) 3×1
B) 1×3
C) 3×3
D) 9×9
回答： 
答案： A


(21) [ 多选 ] ID:16262
inception网络主要是由那几层构成的?（ ） 
A) 卷积层
B) 激活函数
C) 池化层
D) 全连接层FC
回答： 
答案： ABCD


(22) [ 多选 ] ID:16263
从图像中提取CNN特征，( )模型是首选算法。它的缺点在于，参数量有140M之多，需要更( )的存储空间。但是这个模型很有研究价值 
A) VGG
B) 大
C) 小
D) 少
回答： 
答案： AB


(23) [ 多选 ] ID:16260
Inception 网络都是顶尖的网络代表，Inception 细分种类有那几种?（ ） 
A) Inception v1
B) Inception v2
C) Inception v3，Inception v4
D) Inception-ResNet
回答： 
答案： ABCD


(24) [ 多选 ] ID:16556
迁移学习之开发模型的方法包括那几种? 
A) 选择源任务
B) 开发源模型
C) 重用模型
D) 调整模型
回答： 
答案： ABCD


(25) [ 多选 ] ID:16259
（ ）模型是2014年ILSVRC竞赛的第二名，第一名是( )。但是VGG模型在多个迁移学习任务中的表现要优于googLeNet 
A) VGG
B) GoogLeNet
C) ResNet
D) AlexNet
回答： 
答案： AB


(26) [ 多选 ] ID:16267
在深度神经网络中，如果是填充后求卷积，图像尺寸保持不变，以下哪些参数是正确的 
A) 以0填充一圈
B) 以1填充一圈
C) 步长为1
D) 图像尺寸变小
回答： 
答案： AC


(27) [ 多选 ] ID:16261
googLeNet摒弃了（ ）等传统著名网络的“一条线”架构。升级版为Inception 
A) GoogLeNet
B) VGG
C) ResNet
D) AlexNet
回答： 
答案： BD


(28) [ 多选 ] ID:16265
tensorflow中，函数tensorflow.nn.softmax_cross_entropy_with_logits()的功能描述正确的是 
A) 在卷积层
B) 进行优化
C) 用信息熵
D) 一定全连接层
回答： 
答案： BCD


(29) [ 多选 ] ID:16268
关于LeNet深度神经网络的描述，它的组成部分是 
A) 卷积层
B) 激活函数
C) 池化层
D) 全连接层
回答： 
答案： ABCD


(30) [ 多选 ] ID:16557
迁移学习是一种机器学习的方法，包含以下两种开发方法? 
A) 开发模型的方法
B) 预训练模型的方法
C) 数据清洗
D) 似然变换
回答： 
答案： AB


(31) [ 判断 ] ID:16272
LeNet5网络最初是为了识别支票上面的手写数字而设计的 
回答： 
答案： 是


(32) [ 判断 ] ID:16275
信息量就是信息熵。 
回答： 
答案： 否


(33) [ 判断 ] ID:16273
事件的信息量，与事件发生的概率无关。 
回答： 
答案： 否


(34) [ 判断 ] ID:16280
激活函数给神经元引入了非线性因素，使得神经网络可以任意逼近任何非线性函数，这样神经网络就可以应用到众多的非线性模型中 
回答： 
答案： 是


(35) [ 判断 ] ID:16276
relu函数只能在全连接层的神经网络里面有意义，在卷积层没有意义 
回答： 
答案： 否


(36) [ 判断 ] ID:16271
损失函数有两个表达方式，均方差表达，和信息熵表达。 
回答： 
答案： 是


(37) [ 判断 ] ID:16278
当学习率设置的过大时，收敛过程将变得十分缓慢。而当学习率设置的过小时，梯度可能会在最小值附近来回震荡，甚至可能无法收敛 
回答： 
答案： 否


(38) [ 判断 ] ID:16274
分类问题用逻辑回归，预测问题用线性回归。 
回答： 
答案： 是


(39) [ 判断 ] ID:16279
BP算法是由学习过程由信号的正向传播与误差的反向传播两个过程组成。由于多层前馈网络的训练经常采用误差反向传播算法，人们也常把将多层前馈网络直接称为BP网络 
回答： 
答案： 是


(40) [ 判断 ] ID:16281
用二维滤波器在被卷积的2D图像上所有位置滑动，并在每个位置上与对应像素点及其领域像素点做内积，这就是二维卷积的原理 
回答： 
答案： 是


(41) [ 判断 ] ID:16277
softmax用于多分类过程中，它将多个神经元的输出，映射到（0,1）区间内，可以看成概率来理解，从而来进行多分类 
回答： 
答案： 是
