(1) [ 单选 ] ID:16033
在tf.assign(a,b)的意义是： 
A) 将b节点变量赋值给a节点变量
B) 将a节点变量赋值给b节点变量
C) a=b
D) b=a
回答： 
答案： A


(2) [ 单选 ] ID:16425
以下哪个是长短期记忆网络的缩写 
A) LSTM
B) GRU
C) CNN
D) RNN
回答： 
答案： A


(3) [ 单选 ] ID:16428
LSTM中，哪个门的作用是“决定我们会从细胞状态中丢弃什么信息”？ 
A) 输入门
B) 遗忘门
C) 输出门
D) 更新门
回答： 
答案： B


(4) [ 单选 ] ID:16422
长短期记忆网络是RNN的一种变体，RNN由于梯度消失的原因只能有( )记忆 
A) 长期
B) 短期
C) 中期
D) 后期
回答： 
答案： B


(5) [ 单选 ] ID:16545
激活函数tf.nn.relu能 
A) 用于卷积后数据
B) 用于卷积核
C) 用于步长
D) 不能用到全连接层
回答： 
答案： A


(6) [ 单选 ] ID:16292
Vanishing Gradient Problem 问题是（ ），这个问题是在神经网络框架设计中经常出现的问题，例如隐藏层设计过多而导致的。 
A) 梯度消失问题
B) 卷积
C) 池化
D) 全连接
回答： 
答案： A


(7) [ 单选 ] ID:16522
CNN卷积神经网络，RNN循环神经网络，(  )这三个网络都是TensorFlow中支持并常用的经典网络 
A) RNN
B) XNN
C) BNN
D) LSTM长短记忆算法
回答： 
答案： D


(8) [ 单选 ] ID:16426
以下哪个是门控循环单元 
A) LSTM
B) GRU
C) CNN
D) RNN
回答： 
答案： B


(9) [ 单选 ] ID:16524
多层神经网络中使用全连接层的目的是? 
A) 滤波
B) One-hot处理
C) 用于特征提取
D) 用于分类
回答： 
答案： D


(10) [ 单选 ] ID:16447
卷积网络在本质上是一种输入到输出的（ ），它能够学习大量的输入与输出之间的映射关系，而不需要任何输入和输出之间的精确的数学表达式。 
A) 卷积
B) 池化
C) 全连接
D) 映射
回答： 
答案： D


(11) [ 单选 ] ID:16424
将多个LSTM组合成层，网络中有多层，复杂的结构能够处理更大范围的（ ） 
A) 动态性
B) 静态性
C) 不稳定性
D) 鲁棒性
回答： 
答案： A


(12) [ 单选 ] ID:16427
LSTM中，哪个门的作用是“确定哪些新的信息留在细胞状态中，并更新细胞状态”？ 
A) 输入门
B) 遗忘门
C) 输出门
D) 更新门
回答： 
答案： A


(13) [ 单选 ] ID:16456
tf.nn.dropout是TensorFlow里面为了防止或减轻过拟合而使用的函数，它一般用在（ ） 
A) 卷积层
B) 全连接层
C) 池化层
D) 激活函数层
回答： 
答案： B


(14) [ 单选 ] ID:16429
LSTM中，哪个门的作用是“确定输出，把前面的信息保存到隐层中去”？ 
A) 输入门
B) 遗忘门
C) 输出门
D) 更新门
回答： 
答案： C


(15) [ 单选 ] ID:16363
（ ）是一种特定形式的RNN循环神经网络，英文全称Long short-term memory 
A) CNN
B) LSTM
C) RNN
D) CONV
回答： 
答案： B


(16) [ 单选 ] ID:16038
在a=tf.Variable([1，2，3])，b=tf.Variable([3，2，1])，经过tf.multiply(a,b)后值为 
A) [3，4，3]
B) 10
C) 运算不能进行
D) 结果是矩阵
回答： 
答案： A


(17) [ 单选 ] ID:16583
对于图像识别问题（比如识别照片中的猫），神经网络模型结构更适合解决哪类问题？ 
A) 多层感知器
B) 卷积神经网络
C) 循环神经网络
D) 感知器
回答： 
答案： B


(18) [ 单选 ] ID:16526
通常全连接层在卷积神经网络的（） 
A) 前几层
B) 尾部层
C) 中间层
D) 前后几层
回答： 
答案： B


(19) [ 单选 ] ID:16450
在卷积神经网络中，卷积层与全连接层的先后顺序通常为? 
A) 先卷积、池化后全连接
B) 先全连接、卷积后池化
C) 先池化、全连接再卷积
D) 先卷积、全连接、池化后
回答： 
答案： A


(20) [ 单选 ] ID:16423
LSTM网络通过精妙的( )将短期记忆与长期记忆结合起来，并且一定程度上解决了梯度消失的问题 
A) RNN控制
B) 前馈控制
C) BP控制
D) 门控制
回答： 
答案： D


(21) [ 多选 ] ID:16647
哪些是tensorflow的算术运算函数？ 
A) add()
B) subtract()
C) multiply()
D) div()
回答： 
答案： ABCD


(22) [ 多选 ] ID:16431
在循环神经网络中，LSTM有哪几个门？ 
A) 输入门
B) 遗忘门
C) 输出门
D) 更新门
回答： 
答案： ABC


(23) [ 多选 ] ID:16587
哪些是Tensorflow的RNN中关于cell的类 
A) BasicRNNCell
B) BasicLSTMCell
C) GRUCell
D) MultiRNNCell
回答： 
答案： ABCD


(24) [ 多选 ] ID:16432
在循环神经网络中，GRU中有几个门？ 
A) 输入门
B) 遗忘门
C) 重置门
D) 更新门
回答： 
答案： CD


(25) [ 多选 ] ID:16555
如果填充图像后求卷积，并保持图像尺寸不变化 
A) 以0填充
B) 填充厚度为卷积核厚度减1的一半
C) 步长为1
D) 步长大于1
回答： 
答案： ABC


(26) [ 多选 ] ID:16582
在神经网络中，以下哪种技术用于解决过拟合？ 
A) Dropout
B) 正则化
C) 批规范化
D) 激活函数
回答： 
答案： ABC


(27) [ 多选 ] ID:16430
在循环神经网络中，哪些技术可以改善梯度消失问题 
A) LSTM
B) GRU
C) RNN
D) BRNN
回答： 
答案： AB


(28) [ 多选 ] ID:16593
目前，深度学习主要包括（）？ 
A) 前馈神经网络
B) 卷积神经网络
C) 循环神经网络
D) 对抗神经网络
回答： 
答案： ABCD


(29) [ 多选 ] ID:16266
卷积核K的四个重要参数是 
A) 高度
B) 宽度
C) 输入通道
D) 输出通道
回答： 
答案： ABCD


(30) [ 多选 ] ID:16584
什么是影响神经网络的深度选择的因素？ 
A) 神经网络的类型，输入数据
B) 输出函数映射
C) 计算能力，即硬件和软件能力
D) 学习率
回答： 
答案： ABCD


(31) [ 判断 ] ID:16434
关于循环神经网络，GRU中有两个门 
回答： 
答案： 是


(32) [ 判断 ] ID:16585
增大卷积核的大小必然会提高卷积神经网络的性能。 
回答： 
答案： 否


(33) [ 判断 ] ID:16437
RNN的LSTM可以防止梯度消失或者梯度爆炸 
回答： 
答案： 是


(34) [ 判断 ] ID:16581
双向循环神经网络（bidirectional recurrent neural network，Bi-RNN）由两层循环神经网络组成，它们的输入相同，只是信息传递的方向不同。 
回答： 
答案： 是


(35) [ 判断 ] ID:16436
梯度爆炸时，导数值比较大，发生数值溢出，会出现NaN 
回答： 
答案： 是


(36) [ 判断 ] ID:16580
RNN中，LSTM比GRU更加灵活。 
回答： 
答案： 是


(37) [ 判断 ] ID:16438
LSTM（Long short-term memory），主要由四个Component组成：Input Gate，Output Gate，Memory Cell以及Forget Gate 
回答： 
答案： 是


(38) [ 判断 ] ID:16579
GRU是LSTM网络的一种效果很好的变体，它较LSTM网络的结构更加简单。 
回答： 
答案： 是


(39) [ 判断 ] ID:16433
循环神经网络的LSTM中有三个门 
回答： 
答案： 是


(40) [ 判断 ] ID:16230
relu函数只能在全连接层的神经网络里面有意义 
回答： 
答案： 否


(41) [ 判断 ] ID:16435
GRU单元有记忆细胞Cell 
回答： 
答案： 是
