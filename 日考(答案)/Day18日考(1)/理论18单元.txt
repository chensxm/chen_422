(1) [ 单选 ] ID:16458
局部响应标准化，此该环节主要用于防止过拟合，简称是什么？ 
A) 本区域
B) LRN
C) BP算法
D) 池化
回答： 
答案： B


(2) [ 单选 ] ID:16440
如果词汇量是10000，每个词汇由300个特征表示，那么嵌入矩阵就是一个（）的矩阵 
A) 10000*10000
B) 300*300
C) 300*10000
D) 100*10000
回答： 
答案： C


(3) [ 单选 ] ID:16457
( )就是在不同的训练过程中随机扔掉一部分神经元。也就是让某个神经元的激活值以一定的概率p，让其停止工作，这次训练过程中不更新权值，也不参加神经网络的计算。但是它的权重得保留下来（只是暂时不更新而已） 
A) conv2d
B) max_pool
C) Dropout
D) FC
回答： 
答案： C


(4) [ 单选 ] ID:16453
经过激励传播、权重更新这两个环节的反复循环迭代，最终实现网络对输入的响应达到预定的目标值或范围为止，这就是（ ）传播算法 
A) 前向
B) 反向
C) 卷积
D) 填充
回答： 
答案： B


(5) [ 单选 ] ID:16449
相比于其它激活函数来说，ReLU有以下优势：对于线性函数而言，ReLU的表达能力更强，尤其体现在深度网络中；而对于非线性函数而言，ReLU由于非负区间的梯度为常数，因此不存在（ ）(Vanishing Gradient Problem)，使得模型的收敛速度维持在一个稳定状态。 
A) 梯度消失问题
B) 卷积
C) 池化
D) 全连接
回答： 
答案： A


(6) [ 单选 ] ID:16445
在tf.nn.conv2d(a, b, c, d )，其中填充操作是 
A) a
B) b
C) c
D) d
回答： 
答案： D


(7) [ 单选 ] ID:16455
我们会发现累乘会导致激活函数导数的累乘，进而会导致“( )“和“梯度爆炸“现象的发生 
A) 梯度消失
B) 梯度上升
C) 梯度下降
D) 梯度反向
回答： 
答案： A


(8) [ 单选 ] ID:16454
从sigmoid函数的图像中可以看到，如果x稍微大点的话，其值接近为1，则在进行反向传播算法的过程中对其求导的导数非常的接近0，因此会导致梯度为0的（ ）的现象 
A) 梯度消失
B) 梯度上升
C) 梯度下降
D) 梯度发散
回答： 
答案： A


(9) [ 单选 ] ID:16444
对于神经网络，AW = Y，其中A是训练集合，Y是标签集合，总体损失函数E一般表示为是： 
A) 0.5乘（Y-AX）的范数平方
B) （Y-AX）
C) Y对X的偏导数
D) （AX-Y）
回答： 
答案： A


(10) [ 单选 ] ID:16460
( )翻译为“Vanishing Gradient Problem”，这种问题通常是基于梯度的方法训练神经网络的过程中才会出现的。 
A) 梯度消失问题
B) 卷积
C) 池化
D) 全连接
回答： 
答案： A


(11) [ 单选 ] ID:16441
以下哪个余弦相似度值接近1？ 
A) 法国和意大利
B) 球和鳄鱼
C) 体育和动物
D) 法国和足球
回答： 
答案： A


(12) [ 单选 ] ID:16463
（ ）是定义在单个样本上的，是指一个样本的误差 
A) 损失函数（Loss Function）
B) 代价函数（Cost Function）
C) 目标函数（Object Function）
D) 范数
回答： 
答案： A


(13) [ 单选 ] ID:16446
在一个32X32大小的图像，通过步长为1，不考虑填充，大小为5X5的卷积核卷积后，结果尺寸成为 
A) 28X28
B) 14X14
C) 31X31
D) 32X32
回答： 
答案： A


(14) [ 单选 ] ID:16452
（ ）对于人工神经网络模型去学习、理解非常复杂和非线性的函数来说具有十分重要的作用 
A) 激活函数
B) 卷积
C) 池化
D) 全连接
回答： 
答案： A


(15) [ 单选 ] ID:16448
（ ）是定义在整个训练集上的，是所有样本误差的平均，也就是所有损失函数值的平均。 
A) 损失函数（Loss Function）
B) 代价函数（Cost Function）
C) 目标函数（Object Function）
D) 范数
回答： 
答案： B


(16) [ 单选 ] ID:16459
局部响应归一化即( )，LRN一般是在激活、池化后进行的一中处理方法 
A) 本区域
B) local response normalization
C) BP算法
D) 池化
回答： 
答案： B


(17) [ 单选 ] ID:16442
全连接神经网络，如果输入层为32X4矩阵，那么与它相连的第一级参数矩阵最有可能为哪一种?否则无法矩阵运算。 
A) 4X5矩阵
B) 32X4矩阵
C) 32X32矩阵
D) 任意尺寸矩阵
回答： 
答案： A


(18) [ 单选 ] ID:16461
( )是深度学习训练时的一种提高准确度的技术方法。英文全称为“local response normalization” 
A) 本区域
B) 局部响应归一化/标准化
C) BP算法
D) 池化
回答： 
答案： B


(19) [ 单选 ] ID:16462
当权值过大，前面层比后面层梯度变化更快，会引起梯度爆炸问题，就是所谓的（ ） 
A) 梯度爆炸
B) 卷积
C) 池化
D) 全连接
回答： 
答案： A


(20) [ 单选 ] ID:16439
用e表示词向量，根据词嵌入的特性，e(男人)-e(女人)约等于e(国王)-e(?) 
A) 男人
B) 女人
C) 国王
D) 王后
回答： 
答案： D


(21) [ 多选 ] ID:16597
tensorflow中，哪些运算结果为True 
A) tf.equal(1.0,1)
B) tf.equal(1.0,1.0)
C) tf.equal(0, False)
D) tf.equal(1, False)
回答： 
答案： ABC


(22) [ 多选 ] ID:16464
使用one-hot方法表示词汇有什么缺点？ 
A) 每个单词需要用高维向量来表示，而且只有一个数是零，其他都是1，表示冗余，存储量大
B) 每个单词表示的向量相乘都为零（正交），无法表示词汇之间的联系
C) 效率非常高
D) 能够处理非连续型数值特征
回答： 
答案： AB


(23) [ 多选 ] ID:16467
程序语句tf.nn.dropout(x, keep_prob, noise_shape=None, seed=None,name=None) 正确的描述为? 
A) 第一个参数x：指输入。train的时候才是dropout起作用的时候
B) 第二个参数keep_prob: 设置神经元被选中的概率,在初始化时keep_prob是一个占位符
C) noise_shape：干扰形状。 此字段默认是None，表示第一个元素的操作都是独立
D) 第五个参数name：指定该操作的名字
回答： 
答案： ABCD


(24) [ 多选 ] ID:16586
以下哪些词向量之间的差值非常接近？ 
A) 男人，女人
B) 国王，王后
C) 父亲，母亲
D) 父亲，儿子
回答： 
答案： ABC


(25) [ 多选 ] ID:16648
以下哪些tensorflow的函数可以对张量进行逻辑运算 
A) equal()
B) not_equal()
C) less()
D) greater()
回答： 
答案： ABCD


(26) [ 多选 ] ID:16602
以下哪些属于独热编码？ 
A) [0,1,0,0]
B) [0,1,1,0]
C) [1,0,0,0]
D) [0,0,0,1]
回答： 
答案： ACD


(27) [ 多选 ] ID:16468
tf.nn.dropout()是tensorflow里面为了防止或减轻过拟合而使用的函数，它一般用在全连接层，具体的作用是有那些? 
A) 停止某些节点更新
B) 保留百分率为keep_prob的节点更新
C) 在全链接层有效
D) 可以停止任意层节点更新
回答： 
答案： ABCD


(28) [ 多选 ] ID:16465
在《深度学习》网络中，神经网络层的卷积部份一般都有什么层构成? 
A) 卷积层
B) RELU
C) POOLING层
D) pedding
回答： 
答案： ABC


(29) [ 多选 ] ID:16466
池化层是夹在连续的( )之间的层次，其主要的工作是用来对数据进行下采样，从而压缩数据和参数的量，减少( ) 
A) 卷积层
B) 过拟合
C) 池化
D) 正则化
回答： 
答案： AB


(30) [ 多选 ] ID:16592
比较适合循环神经网络的应用有哪些？ 
A) 视频行为识别
B) 实体名字识别
C) 语音识别
D) 机器翻译
回答： 
答案： ABCD


(31) [ 判断 ] ID:16473
在寻优的迭代过程中，损失函数的运算结果，从趋势上是逐步降低的。 
回答： 
答案： 是


(32) [ 判断 ] ID:16479
函数tf.nn.softmax_cross_entropy_with_logits_v2是常用的最新版本的交叉熵函数 
回答： 
答案： 是


(33) [ 判断 ] ID:16472
梯度和损失函数一样，也是数量函数 
回答： 
答案： 否


(34) [ 判断 ] ID:16482
Padding（填充）属性定义元素边框与元素内容之间的空间 
回答： 
答案： 是


(35) [ 判断 ] ID:16469
损失函数，可以由误差向量的总体范数平方定义 
回答： 
答案： 是


(36) [ 判断 ] ID:16478
损失函数（loss function）是用来估量模型的预测值f(x)与真实值Y的不一致程度，它是一个非负实值函数,通常使用L(Y, f(x))来表示，损失函数越大，模型的鲁棒性就越好。 
回答： 
答案： 否


(37) [ 判断 ] ID:16480
BP网络的输入输出关系实质上是一种映射关系：一个n输入m输出的BP神经网络所完成的功能是从n维欧氏空间向m维欧氏空间中一有限域的连续映射，这一映射具有高度非线性。 
回答： 
答案： 是


(38) [ 判断 ] ID:16477
sigmoid函数是一个良好的阈值函数，连续，光滑，严格单调 
回答： 
答案： 是


(39) [ 判断 ] ID:16474
均方损失函数，是个数量函数，它的张量的维度是0 
回答： 
答案： 是


(40) [ 判断 ] ID:16471
对平方范数的损失函数求偏导数的结果，得到一个梯度向量。 
回答： 
答案： 是


(41) [ 判断 ] ID:16481
均方损失函数，是个数量函数，张量维度为1 
回答： 
答案： 否


(42) [ 判断 ] ID:16476
激活函数也就是大于某个值输出1（被激活了），小于等于则输出0（没有激活）。这个函数是非线性函数。 
回答： 
答案： 是


(43) [ 判断 ] ID:16470
损失函数，是个数量函数，张量维度为0 
回答： 
答案： 是


(44) [ 判断 ] ID:16475
全连接层无法实现卷积运算，他们互有分工。 
回答： 
答案： 是
