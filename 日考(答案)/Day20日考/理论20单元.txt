(1) [ 单选 ] ID:16517
函数tf.nn.relu的作用是? 
A) 被卷积数据
B) 卷积核
C) 步长
D) 激活
回答： 
答案： D


(2) [ 单选 ] ID:16519
池化核ksize=[1, 2, 2, 1]将图像 
A) 缩小到1/2
B) 缩小到1/4
C) 扩大两倍
D) 扩大四倍
回答： 
答案： B


(3) [ 单选 ] ID:16525
语句tf.nn.conv2d()，其中遇到的图像张量，格式是 
A) [batch, in_height, in_width, in_channels]
B) [Size, in_height, in_width, in_channels]
C) [batch, in_width, in_height， in_channels]
D) [batch, in_channels，in_height, in_width ]
回答： 
答案： A


(4) [ 单选 ] ID:16549
vanishing gradient problem 是指在BP算法的过程中，error项逐渐变小，使得越靠前的网络层的学习速率越来越低 
A) 梯度上升问题
B) 梯度优化
C) 梯度消失问题
D) 梯度下降法
回答： 
答案： C


(5) [ 单选 ] ID:16211
在深度神经网络中，全连接层的作用是 
A) 滤波
B) One-hot处理
C) 用于特征提取
D) 用于分类
回答： 
答案： D


(6) [ 单选 ] ID:16655
关于循环神经网络，哪个不是LSTM的门？ 
A) 输入门
B) 遗忘门
C) 输出门
D) 更新门
回答： 
答案： D


(7) [ 单选 ] ID:16518
conv2d(x_image, W_conv1) + b_conv1 
A) 对图像池化
B) 卷积函数
C) 激活函数
D) 平均
回答： 
答案： B


(8) [ 单选 ] ID:16516
情感分类属于哪一类问题？ 
A) 多个输入多个输出
B) 一个输入多个输出
C) 一个输入一个输出
D) 多个输入一个输出
回答： 
答案： D


(9) [ 单选 ] ID:16523
程序语句 max_pool(conv2, [1, 3, 3, 1], strides=[1, 1, 1, 1], padding="SAME")那么这个语句中，体现的池化窗口尺寸为多少? 
A) 1*1
B) 3*3
C) 1*3
D) 3*1
回答： 
答案： B


(10) [ 单选 ] ID:16548
以下tf函数中，不能实现卷积运算的是 
A) tf.nn.conv2d
B) tf.nn.depthwise_conv2d
C) tf.nn.convolution
D) tf.random_normal
回答： 
答案： D


(11) [ 单选 ] ID:16520
步长张量strides=[1, 2, 2, 1]能横向纵向移动 
A) 1像素
B) 2像素
C) 3像素
D) 4像素
回答： 
答案： B


(12) [ 单选 ] ID:16650
f = tf.Variable([[2., 5., 4.], [1., 3., 6.]]), tf.reduce_sum(f, axis=1)的值是 
A) [10., 11.]
B) [10., 10.]
C) [11., 11.]
D) [11., 10.]
回答： 
答案： D


(13) [ 单选 ] ID:16644
如果x的值是True，那么tf.cast(x, tf.float32)的值是什么？ 
A) 0.0
B) 1.0
C) False
D) True
回答： 
答案： B


(14) [ 单选 ] ID:16595
输入大小为64X64的黑白图像，卷积核5X5，步长为1，填充方式为“VALID”，卷积后图像尺寸为 
A) 59
B) 60
C) 58
D) 61
回答： 
答案： B


(15) [ 单选 ] ID:16596
tensorflow的张量f的值为[[2., 5., 4.], [1., 3., 6.]]，那么tf.argmax(f, axis=0)的结果是？ 
A) [[5],[6]]
B) [1,2]
C) [0, 0, 1]
D) [[2],[5],[6]]
回答： 
答案： C


(16) [ 单选 ] ID:16521
现有一个32X32大小的图像，通过步长为1，填充p=1，大小为5X5的卷积核卷积后，结果尺寸成为 
A) 28X28
B) 30X30
C) 31X31
D) 32X32
回答： 
答案： B


(17) [ 单选 ] ID:16035
tf中placeholder是一个 
A) 常量
B) 变量
C) 占位符
D) 函数
回答： 
答案： C


(18) [ 单选 ] ID:16598
tensorflow中，tensor = tf.constant([1, 2, 3, 4, 5, 6, 7,8])，sess.run(tf.reshape(tensor,[2,-1]))运行后，tensor的内容变成？ 
A) [[1,2,3,4],[5,6,7,8]]
B) [[1,2,3], [4,5,6,7,8]]
C) [[1,2,3], [4,5,6],[7,8]]
D) [[1,2],[3,4],[5,6],[7,8]]
回答： 
答案： A


(19) [ 单选 ] ID:16659
（）是指根据文本所表达的含义和情感信息将文本划分成褒扬的或贬义的两种或几种类型，是对文本作者倾向性和观点、态度的划分，因此有时也称倾向性分析。 
A) 语音识别
B) 机器学习
C) 自然语言处理
D) 情感分类
回答： 
答案： D


(20) [ 单选 ] ID:16654
双向循环神经网络的英文缩写是？ 
A) RNN
B) SRNN
C) TRNN
D) Bi-RNN
回答： 
答案： D


(21) [ 多选 ] ID:16527
程序语句例如conv2d(input_d,filter_d,strides=[1,3,3,1],padding='SAME')，这条语句的含意理解为（ ）? 
A) 步长在高度方向和宽度方向均为3
B) 填充图像边缘，使图像尺寸不变
C) input_d是待卷积的数据
D) 进行卷积操作
回答： 
答案： ABCD


(22) [ 多选 ] ID:16552
属于卷积核K的参数是 
A) 步长
B) 填充
C) 输入通道
D) 输出通道
回答： 
答案： CD


(23) [ 多选 ] ID:16530
图像识别常用的方案有那些? 
A) 人脸检测
B) 表情判断
C) 动作识别
D) 无人驾驶
回答： 
答案： ABCD


(24) [ 多选 ] ID:16531
深度神经网络的构成中，把卷积层放在前面，全连接层放在后面，它们的作用是什么？ 
A) 用卷积层提取特征
B) pooling的下采样能够降低overfitting
C) 激活函数relu可以用到卷积层
D) 全连接层只能有一层
回答： 
答案： ABC


(25) [ 多选 ] ID:16558
手写体识别程序经常调用的四个数据集包括以下选项中那几个?（） 
A) train-images.idx3-ubyte
B) train-labels-idx1-ubyte
C) t10k-images.idx3-ubyte
D) t10k-labels-idx1-ubyte
回答： 
答案： ABCD


(26) [ 多选 ] ID:16554
下面几个参数中，不属于卷积核的参数是 
A) 步长
B) 填充
C) 输入通道
D) 输出通道
回答： 
答案： AB


(27) [ 多选 ] ID:16532
以下哪条tf语句能够描述损失函数 
A) loss = tf.reduce_mean(tf.square(y - y_data))
B) loss = tf.reduce_sum(tf.square(y - y_data))
C) loss = tf.reduce_mean(tf.add(y，y_data))
D) loss = tf.reduce_mean(tf.subtract(y，y_data))
回答： 
答案： AB


(28) [ 多选 ] ID:16528
定义卷积核W_conv1 = weight_variable([5, 5, 5, 32])后 
A) 尺寸5X5
B) 输入通道5
C) 输出通道32
D) 有32个卷积核
回答： 
答案： ABCD


(29) [ 多选 ] ID:16553
如果是填充后求卷积，图像尺寸不变化 
A) 以0填充
B) 填充厚度为卷积核厚度减1的一半
C) 步长为1
D) 图像尺寸变小
回答： 
答案： ABC


(30) [ 多选 ] ID:16529
在神经网络中，常常会用到如下函数：tensorflow.nn.softmax_cross_entropy_with_logits_V2(）是 
A) 在卷积层
B) 进行优化
C) 用信息熵
D) 一定全连接层
回答： 
答案： BCD


(31) [ 判断 ] ID:16533
常用用的池化方式为mean-pooling，max-pooling 
回答： 
答案： 是


(32) [ 判断 ] ID:16560
用sigmoid函数不能将运算映射到概率空间。 
回答： 
答案： 否


(33) [ 判断 ] ID:16567
处理分类问题一般用神经网络，处理预测问题用线性回归。 
回答： 
答案： 是


(34) [ 判断 ] ID:16534
卷积神经网络中，其中，卷积层是分类的，全连接层是特征提取的。 
回答： 
答案： 否


(35) [ 判断 ] ID:16538
sigmoid函数也能将全数域函数，映射到概率空间。 
回答： 
答案： 是


(36) [ 判断 ] ID:16535
relu 是一个激活函数 
回答： 
答案： 是


(37) [ 判断 ] ID:16559
归一化指数函数Softmax函数，是用来将运算结果映射到概率空间。 
回答： 
答案： 是


(38) [ 判断 ] ID:16564
在使用SOFTMAX函数时，是将运算结果从向量空间转化到概率空间。 
回答： 
答案： 是


(39) [ 判断 ] ID:16539
损失函数是经验风险函数的核心部分，也是结构风险函数重要组成部分 
回答： 
答案： 是


(40) [ 判断 ] ID:16563
就目前学习，损失函数有均方差表达，和交叉熵表达。 
回答： 
答案： 是


(41) [ 判断 ] ID:16568
在一个系统中，信息量的概率平均就是信息熵。 
回答： 
答案： 是


(42) [ 判断 ] ID:16566
概率系统中，单个事件的信息熵，是系统信息熵的一个组成部分。 
回答： 
答案： 是


(43) [ 判断 ] ID:16540
全连接层无法实现卷积运算。 
回答： 
答案： 是


(44) [ 判断 ] ID:16561
事件的信息熵，是系统信息熵的一个组成部分。 
回答： 
答案： 是


(45) [ 判断 ] ID:16562
分类问题用神经网络，预测问题用线性回归。 
回答： 
答案： 是


(46) [ 判断 ] ID:16536
在深度神经网络中，通常使用一种叫修正线性单元(Rectified linear unit，ReLU）作为神经元的激活函数 
回答： 
答案： 是


(47) [ 判断 ] ID:16537
学习率(Learning rate)作为监督学习以及深度学习中重要的超参，其决定着目标函数能否收敛到局部最小值以及何时收敛到最小值。合适的学习率能够使目标函数在合适的时间内收敛到局部最小值。 
回答： 
答案： 是


(48) [ 判断 ] ID:16565
理论上用sigmoid函数也能将运算映射到概率空间。 
回答： 
答案： 是
